{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326226b7-2e34-4893-acd5-4a8a826f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8fc68d-339b-497a-b490-917cfa7572b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TTS:\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         self.Sentance = self.UserInput()\n",
    "        \n",
    "#         self.Normalized = self.Normalization(self.Sentance)\n",
    "        \n",
    "#         self.Tokenized = self.Tokenization(self.Normalized)\n",
    "        \n",
    "#         self.SentanceSyllables = self.Syllables(self.Tokenized)\n",
    "#         # self.TaggedSentance = self.Tagging(self.Tokenized)\n",
    "        \n",
    "        \n",
    "#     def __str__(self):\n",
    "#         return \"Sentance :{}\\nNormalized Sentance :{}\\nTokenizedText :{})\".format(self.Sentance,self.Normalized,self.Tokenized)\n",
    "        \n",
    "#     def UserInput(self):\n",
    "        \n",
    "#         # for the time being store the sentance in files and read from that file\n",
    "#         try: \n",
    "#             try: \n",
    "                \n",
    "#                 Sent = open(\"UserInput.txt\" ,mode = 'r' , encoding='utf-8').read()\n",
    "#                 return Sent\n",
    "#             except:\n",
    "#                 print(\"UserInput file is empty \")\n",
    "\n",
    "#         except:\n",
    "            \n",
    "#             print(\"Inputs are not defined in UserInput().\")\n",
    "            \n",
    "#     def Normalization(self,Sentance ,encoding='utf-8'):\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # case 1 [Remove English character from Sentance]\n",
    "#         DeletedEngChar = re.sub('[a-zA-Z]' ,\"\",Sentance)\n",
    "        \n",
    "#         # case 2 [Remove Special character from DeletedEngChar]\n",
    "        \n",
    "#         DeletedSpecialChar = re.sub( '[~!@#$%^&*()-_+]' , \"\",DeletedEngChar )\n",
    "        \n",
    "#         # Can add more Usecases it here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         # case 3 [Replace Counting digit to Pashto couting words]\n",
    "        \n",
    "#         MathDigit = ['1','2','3','4','5','6','7','8','9','0']\n",
    "        \n",
    "#         # iterate over all Math digit and replace with it\n",
    "#         pasto_counting_dict = {\n",
    "#            '1': \"يو\"\n",
    "#              ,\n",
    "#              '2': \"دؤه\",\n",
    "#              '3':\"    درے\",\n",
    "#              '4':\"څلور\",\n",
    "#              '5':\"    پنځه\",\n",
    "#              '6':\"شپږ\",\n",
    "#              '7':\"أوؤه\",\n",
    "#              '8':\"    أته\",\n",
    "#              '9':\"نهه\",\n",
    "#              '0' : \"صفر\"}\n",
    "        \n",
    "        \n",
    "#         for digit in MathDigit:\n",
    "#             DeletedSpecialChar = DeletedSpecialChar.replace(digit ,pasto_counting_dict[digit] )\n",
    "         \n",
    "#         return DeletedSpecialChar\n",
    "        \n",
    "#     def Tokenization(self,Sentance):\n",
    "        \n",
    "        \n",
    "#         # case 1 [Removed extra spavce from the Sentance]\n",
    "        \n",
    "#         FreeExtraSpaceText= Sentance.strip()\n",
    "        \n",
    "\n",
    "#         # case 2 [Tokenization] from Python lib's\n",
    "        \n",
    "#         TokenizedText = word_tokenize(FreeExtraSpaceText)\n",
    "        \n",
    "        \n",
    "#         return TokenizedText\n",
    "    \n",
    "    \n",
    "#     def Tagging(self,TokenizeTextList):\n",
    "        \n",
    "#         pass\n",
    "#         # for word in TokenizeTextList:\n",
    "#             # print(word)\n",
    "            \n",
    "            \n",
    "#     def Syllables(self,SentanceTokens):\n",
    "        \n",
    "#         for index , token in enumerate(SentanceTokens):\n",
    "#             # print(index,token)\n",
    "#             print(\"token : {} has IPA : {}\".format(token,self.IpaOfToken(token)))\n",
    "    \n",
    "#     def IpaOfToken(self,token):\n",
    "        \n",
    "#         tokenIpalist = []\n",
    "        \n",
    "#         for char in token:\n",
    "#             CharIPA = self.IPA(char)\n",
    "#             # print(\"Char : {}\\nIPA : {}\".format(char,CharIPA) )\n",
    "#             tokenIpalist.append(CharIPA)\n",
    "            \n",
    "#         return tokenIpalist\n",
    "            \n",
    "        \n",
    "#     def IPA(self,TokenChar):\n",
    "#         df = pd.read_csv(\"Datasets/IPA_pashto.csv\",encoding=\"utf-8\")\n",
    "\n",
    "#         Final = list(df[\"Final\"])\n",
    "#         Medial = list(df[\"Medial\"])\n",
    "#         Initial = list(df[\"Initial\"])\n",
    "#         Isolated= list(df[\"Isolated\"])\n",
    "#         IPA = list(df[\"IPA\"])\n",
    "        \n",
    "        \n",
    "#         IPA_dic = {}\n",
    "    \n",
    "#         for index in range(0,len(IPA)):\n",
    "#             IPA_dic[Isolated[index]] = IPA[index]\n",
    "#             IPA_dic[Initial[index]] = IPA[index]\n",
    "#             IPA_dic[Medial[index]] = IPA[index]\n",
    "#             IPA_dic[Final[index]] = IPA[index]\n",
    "            \n",
    "            \n",
    "            \n",
    "#         if TokenChar in IPA_dic:\n",
    "#             return IPA_dic[TokenChar]\n",
    "#         else:\n",
    "#             f = open(\"Datasets/not_available_ipa.txt\",\"+w\")\n",
    "#             f.write(TokenChar)\n",
    "#             f.close()\n",
    "#             print(\"{} Not present :\".format(TokenChar))\n",
    "#         return\n",
    "\n",
    "        \n",
    "\n",
    "# tts = TTS()\n",
    "# print(tts)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09ee622-844f-4987-ae1c-558b004de157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linguistic:\n",
    "    def __init__(self,filePath):\n",
    "        self.filePath = filePath\n",
    "    \n",
    "    def IPA(self,token_list):\n",
    "        filledIPA = []\n",
    "        for token in token_list:\n",
    "            IPAString = \"\"\n",
    "            for char in token:\n",
    "                # print(\"Char : \",char)\n",
    "                # print(\"type : \",type(char))\n",
    "                ret = self.df_ipa(char)\n",
    "                # print(ret)\n",
    "                \n",
    "                print(\"Token : {} ,char : {} , IPA : {} \".format(token,char,ret))\n",
    "                IPAString = IPAString+str(ret)\n",
    "            filledIPA.append(IPAString)\n",
    "            \n",
    "        # print('filledIPA : ',filledIPA)\n",
    "        return filledIPA\n",
    "            \n",
    "        \n",
    "    def df_ipa(self,TokenChar):\n",
    "        '''\n",
    "            There are three type dictonary Final, medial and initial and \n",
    "        '''\n",
    "        df = pd.read_csv(\"Datasets/IPA_pashto.csv\",encoding=\"utf-8\")\n",
    "        \n",
    "        array_isolated = np.array(df['Isolated'])\n",
    "        array_Final = np.array(df['Final'])\n",
    "        array_Medial = np.array(df['Medial'])\n",
    "        array_Initial = np.array(df['Initial'])\n",
    "        \n",
    "        # general array combined with everyone\n",
    "        IPA = np.array(df['IPA'])\n",
    "        \n",
    "        \n",
    "        # dict1 for isolated\n",
    "        Isolated_dict = {}\n",
    "        Final_dict = {}\n",
    "        Medial_dict = {}\n",
    "        Initial_dict = {}\n",
    "        try :\n",
    "            \n",
    "            if len(array_isolated) == len(IPA) and  len(array_Final) == len(IPA) and len(array_Medial) == len(IPA) and len(array_Initial) == len(IPA):\n",
    "\n",
    "                for index in range(0,len(IPA)):\n",
    "                    # for isolated dictionary\n",
    "                    Isolated_dict[ array_isolated[index].strip(' ')   ] = IPA[index].strip('')\n",
    "\n",
    "                    # for Final dictionary \n",
    "                    Final_dict[array_Final[index].strip(' ')] = IPA[index].strip(' ')\n",
    "\n",
    "                    # for middle dictionary \n",
    "                    Medial_dict[array_Medial[index].strip(' ')] = IPA[index].strip(' ')\n",
    "\n",
    "                    # for initial dictionary\n",
    "                    Initial_dict[array_Initial[index].strip(' ')] = IPA[index].strip(' ')\n",
    "\n",
    "                    # for\n",
    "\n",
    "\n",
    "                # print(\"Isolated Dict : \",Isolated_dict)\n",
    "                # print(\"Final Dict : \",Final_dict)\n",
    "                # print(\"Medial Dict : \",Medial_dict)\n",
    "                # print(\"Initial Dict : \",Initial_dict)\n",
    "        except:\n",
    "            print(\"Key value must be equal in df_ipa function \")\n",
    "        # print(\"Given char \",TokenChar)\n",
    "        # print(\"return char : \",Initial_dict[TokenChar])\n",
    "        # print(\"key \",Isolated_dict.keys())\n",
    "        # print(\"value\",Isolated_dict.values())\n",
    "        \n",
    "        \n",
    "        if TokenChar in Initial_dict:\n",
    "            if Initial_dict[TokenChar] != None:\n",
    "                # print(\"init cond\")\n",
    "                return Initial_dict[TokenChar]\n",
    "            \n",
    "        elif TokenChar in Final_dict:\n",
    "            if Final_dict[TokenChar] != None:\n",
    "                # print(\"Final cond\")\n",
    "                return Final_dict[TokenChar]\n",
    "        elif TokenChar in Medial_dict:\n",
    "            if Medial_dict[TokenChar] != None:\n",
    "                # print(\"Medial cond\")\n",
    "                return Medial_dict[TokenChar]\n",
    "                \n",
    "        elif TokenChar in Isolated_dict:\n",
    "            if Isolated_dict[TokenChar]!= None:\n",
    "                # print(\"Isoalted cond\")\n",
    "                return Isolated_dict[TokenChar]\n",
    "        else:\n",
    "            with open('NotavailableIPA.txt' , 'w') as file:\n",
    "                print('wrote in file')\n",
    "                file.write(TokenChar)\n",
    "                Isolated_dict[TokenChar] = 'r'\n",
    "            return Isolated_dict[TokenChar]\n",
    "\n",
    "        \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    # pass\n",
    "    obl = linguistic('/NotavailableIPA.txt')\n",
    "    # obl.IPA(['څرګندونې', 'د', 'افغانستان', 'لپاره', 'د', 'مرستو', 'ټولولو', 'کنفرانس', 'ته', 'په', 'خبرو', 'کې', 'کولې', 'د', 'افغانستان', 'لپاره', 'د', 'نړیوالو', 'مرستو', 'د', 'راټولولو', 'په', 'موخه', 'دا', 'نړیواله', 'غونډه', 'جينېوا', 'کې', 'جوړه', 'شوې', 'ده', 'د', 'ملګرو', 'ملتونو', 'سرمنشي', 'زیاتوي', 'د', 'ملګرو', 'ملتونو', 'د', 'غوښتل', 'شویو', 'څلور', 'عشاریه', 'څلور', 'ميلیارد', 'ډالرو', 'یوازې', '۱۳', 'سلنه', 'پيسې', 'تر', 'لاسه', 'شوې', 'دي', 'ښاغلي', 'ګوتېرېش', 'پر', 'طالبانو', 'نېوکه', 'وکړه', 'چې', 'نجونې', 'یې', 'له', 'شپږمو', 'ټولګیو', 'پورته', 'له', 'زدکړو', 'را', 'ګرځولې', 'دي', 'خو', 'ټینګار', 'یې', 'دا', 'و', 'چې', 'نړې', 'دې', 'افغانستان', 'له', 'پامه', 'نه', 'غورځوي', 'ملګري', 'ملتونه', 'وايي', 'د', 'افغانستان', '۹۵', 'سلنه', 'خلک', 'کافي', 'خواړه', 'نه', 'لري', 'او', 'نړیوالو', 'بنسټونه', 'د', 'دغه', 'هېواد', 'بشري', 'حالت', 'ناورین', 'بللی', 'دی', 'بریتانیا', 'افغانستان', 'کې', 'د', 'بشري\\u200c', 'ناورین', 'مخنیوي', 'لپاره', 'مرستې', 'وغوښتې', 'ملګري', 'ملتونه', 'د', 'بشري', 'مرستو', 'ځنډېدل', 'به', 'افغانستان', 'کې', 'ستونزې', 'لا', 'ډېرې', 'کړي', 'د', 'افغانستان', 'بشري', 'کړکېچ؛', 'لسګونه', 'ماشومان', 'له', 'شري', 'او', 'خوارځواکۍ', 'مړه', 'شوي'])\n",
    "    # print(obl.df_ipa('ﺭ'))\n",
    "    # print(type('ﺭ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c539a23-0537-4ef4-9d8e-1e7bd39c8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProcessing:\n",
    "    \n",
    "    def __init__(self,OrginalText):\n",
    "        self.OrginalText = OrginalText\n",
    "        # print(\"Orginal Text :{} \".format(self.OrginalText))\n",
    "        \n",
    "        \n",
    "    def Cleaning(self,Sentance):\n",
    "        \n",
    "        # case 0 : remove extra space if exist \n",
    "        # RES = Removed Extra spaceed\n",
    "        RES = Sentance.strip()\n",
    "        \n",
    "        \n",
    "        # case 2 : remove English character if exist :\n",
    "        # REC  = Removed English Character\n",
    "        REC = re.sub('[a-zA-Z]' ,\"\",RES)\n",
    "        \n",
    "        # case 3 :  Remove special character:\n",
    "        # RSC = Removed Special characters\n",
    "        RSC = re.sub( '[~!@#$%^&*()-_+]' , \"\",REC )\n",
    "        \n",
    "        # console print \n",
    "        # print(\"Cleaned Text : {} \".format(RSC))\n",
    "        \n",
    "        return RSC\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def Tokenization(self,Sentance):\n",
    "        # case 1 [Tokenization] from Python lib's\n",
    "        \n",
    "        \n",
    "        TokenizedText = word_tokenize(Sentance)\n",
    "        \n",
    "        # print(\"Tokenized Text : {}\".format(TokenizedText))\n",
    "        \n",
    "        return TokenizedText\n",
    "    \n",
    "    \n",
    "    def Normalization(sefl,sentance):\n",
    "        \n",
    "        # case 3 [Replace Counting digit to Pashto couting words]\n",
    "        \n",
    "        MathDigit = ['1','2','3','4','5','6','7','8','9','0']\n",
    "        \n",
    "        # iterate over all Math digit and replace with it\n",
    "        pasto_counting_dict = {\n",
    "           '1': \"يو\",\n",
    "             '2': \"دؤه\",\n",
    "             '3':\"درے\",\n",
    "             '4':\"څلور\",\n",
    "             '5':\"پنځه\",\n",
    "             '6':\"شپږ\",\n",
    "             '7':\"أوؤه\",\n",
    "             '8':\"أته\",\n",
    "             '9':\"نهه\",\n",
    "             '0' : \"صفر\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for digit in MathDigit:\n",
    "            sentance = sentance.replace(digit ,pasto_counting_dict[digit] )\n",
    "        # print(\"Normalized Text : {}\".format(sentance))\n",
    "        \n",
    "        return sentance\n",
    "    def Testing(self,Text):\n",
    "        \n",
    "        # case 1 clean it \n",
    "        CleanedText = self.Cleaning(Text)\n",
    "        \n",
    "        # case 2 : Normalization\n",
    "        NormalaizedText = self.Normalization(CleanedText)\n",
    "        \n",
    "        # case 3 Tokenization\n",
    "        \n",
    "        TokenizedText = self.Tokenization(NormalaizedText)\n",
    "        \n",
    "        # print(\"Final Text : \",TokenizedText)\n",
    "        \n",
    "        self.IPA(TokenizedText)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    \n",
    "\n",
    "    \n",
    "# randomINput = \"عقل ګودرغاړهاولاړشو غاړه  افلاكو علم انشاء علم\"\n",
    "# ProObject = TextPreProcessing(randomINput)\n",
    "# ProObject.Testing(ProObject.OrginalText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff73618-c736-46e0-98e2-9eaed3e0e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTS(TextPreProcessing ,linguistic):\n",
    "    def __init__(self,text):\n",
    "        TextPreProcessing.__init__(self,text)\n",
    "        \n",
    "    def Testing(self,Text):\n",
    "        \n",
    "        # case 1 clean it \n",
    "        CleanedText = self.Cleaning(Text)\n",
    "        \n",
    "        # case 2 : Normalization\n",
    "        NormalaizedText = self.Normalization(CleanedText)\n",
    "        \n",
    "        # case 3 Tokenization\n",
    "        \n",
    "        TokenizedText = self.Tokenization(NormalaizedText)\n",
    "        \n",
    "        # print(\"Final Text : \",TokenizedText)\n",
    "        \n",
    "        # print(self.IPA(TokenizedText))\n",
    "        \n",
    "        self.Stats(self.IPA(TokenizedText))\n",
    "        \n",
    "        \n",
    "    def Stats(self,IPA):\n",
    "        \n",
    "        print('integrated IPA : ',IPA)\n",
    "        vowels = ['a' ,'ā','ə','i','e','ai','əi']\n",
    "        consonant = ['b','p' ,'t̪','ʈ','s','d͡ʒ','t͡ʃ','h' , 'x','t͡s','d͡z' ,'d̪' ,'ɖ','z','r','ɻ','z','d͡z','ʐ','s','ʃ','x','s','d̪' ,'t̪','z','ɣ','f','q','k','ɡ','l','m','n','ɳ','w','h',]\n",
    "        single_gram = np.zeros((len(consonant) ,  len(vowels)))\n",
    "        \n",
    "        for index in range(0 , len(IPA)):\n",
    "            \n",
    "            for count ,char in enumerate(range(0,len(IPA[index]))):\n",
    "                print(count ,IPA[index][char])\n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e71fab7-1828-426a-9bca-e98641c04aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "څرګندونې د افغانستان لپاره د مرستو ټولولو کنفرانس ته په خبرو کې کولې\n",
      "Token : څرګندونې ,char : څ , IPA : t͡s \n",
      "wrote in file\n",
      "Token : څرګندونې ,char : ر , IPA : r \n",
      "Token : څرګندونې ,char : ګ , IPA : ɡ \n",
      "Token : څرګندونې ,char : ن , IPA : n \n",
      "Token : څرګندونې ,char : د , IPA : d̪ \n",
      "Token : څرګندونې ,char : و , IPA : w \n",
      "Token : څرګندونې ,char : ن , IPA : n \n",
      "Token : څرګندونې ,char : ې , IPA : e \n",
      "Token : د ,char : د , IPA : d̪ \n",
      "Token : افغانستان ,char : ا , IPA : ɑ \n",
      "Token : افغانستان ,char : ف , IPA : f  \n",
      "Token : افغانستان ,char : غ , IPA : ɣ \n",
      "Token : افغانستان ,char : ا , IPA : ɑ \n",
      "Token : افغانستان ,char : ن , IPA : n \n",
      "Token : افغانستان ,char : س , IPA : s \n",
      "Token : افغانستان ,char : ت , IPA : t̪ \n",
      "Token : افغانستان ,char : ا , IPA : ɑ \n",
      "Token : افغانستان ,char : ن , IPA : n \n",
      "Token : لپاره ,char : ل , IPA : l \n",
      "Token : لپاره ,char : پ , IPA : p \n",
      "Token : لپاره ,char : ا , IPA : ɑ \n",
      "wrote in file\n",
      "Token : لپاره ,char : ر , IPA : r \n",
      "Token : لپاره ,char : ه , IPA : h \n",
      "Token : د ,char : د , IPA : d̪ \n",
      "Token : مرستو ,char : م , IPA : m \n",
      "wrote in file\n",
      "Token : مرستو ,char : ر , IPA : r \n",
      "Token : مرستو ,char : س , IPA : s \n",
      "Token : مرستو ,char : ت , IPA : t̪ \n",
      "Token : مرستو ,char : و , IPA : w \n",
      "Token : ټولولو ,char : ټ , IPA : ʈ \n",
      "Token : ټولولو ,char : و , IPA : w \n",
      "Token : ټولولو ,char : ل , IPA : l \n",
      "Token : ټولولو ,char : و , IPA : w \n",
      "Token : ټولولو ,char : ل , IPA : l \n",
      "Token : ټولولو ,char : و , IPA : w \n",
      "Token : کنفرانس ,char : ک , IPA : k \n",
      "Token : کنفرانس ,char : ن , IPA : n \n",
      "Token : کنفرانس ,char : ف , IPA : f  \n",
      "wrote in file\n",
      "Token : کنفرانس ,char : ر , IPA : r \n",
      "Token : کنفرانس ,char : ا , IPA : ɑ \n",
      "Token : کنفرانس ,char : ن , IPA : n \n",
      "Token : کنفرانس ,char : س , IPA : s \n",
      "Token : ته ,char : ت , IPA : t̪ \n",
      "Token : ته ,char : ه , IPA : h \n",
      "Token : په ,char : پ , IPA : p \n",
      "Token : په ,char : ه , IPA : h \n",
      "Token : خبرو ,char : خ , IPA : x \n",
      "Token : خبرو ,char : ب , IPA : b \n",
      "wrote in file\n",
      "Token : خبرو ,char : ر , IPA : r \n",
      "Token : خبرو ,char : و , IPA : w \n",
      "Token : کې ,char : ک , IPA : k \n",
      "Token : کې ,char : ې , IPA : e \n",
      "Token : کولې ,char : ک , IPA : k \n",
      "Token : کولې ,char : و , IPA : w \n",
      "Token : کولې ,char : ل , IPA : l \n",
      "Token : کولې ,char : ې , IPA : e \n",
      "integrated IPA :  ['t͡srɡnd̪wne', 'd̪', 'ɑf ɣɑnst̪ɑn', 'lpɑrh', 'd̪', 'mrst̪w', 'ʈwlwlw', 'knf rɑns', 't̪h', 'ph', 'xbrw', 'ke', 'kwle']\n",
      "0 t\n",
      "1 ͡\n",
      "2 s\n",
      "3 r\n",
      "4 ɡ\n",
      "5 n\n",
      "6 d\n",
      "7 ̪\n",
      "8 w\n",
      "9 n\n",
      "10 e\n",
      "0 d\n",
      "1 ̪\n",
      "0 ɑ\n",
      "1 f\n",
      "2  \n",
      "3 ɣ\n",
      "4 ɑ\n",
      "5 n\n",
      "6 s\n",
      "7 t\n",
      "8 ̪\n",
      "9 ɑ\n",
      "10 n\n",
      "0 l\n",
      "1 p\n",
      "2 ɑ\n",
      "3 r\n",
      "4 h\n",
      "0 d\n",
      "1 ̪\n",
      "0 m\n",
      "1 r\n",
      "2 s\n",
      "3 t\n",
      "4 ̪\n",
      "5 w\n",
      "0 ʈ\n",
      "1 w\n",
      "2 l\n",
      "3 w\n",
      "4 l\n",
      "5 w\n",
      "0 k\n",
      "1 n\n",
      "2 f\n",
      "3  \n",
      "4 r\n",
      "5 ɑ\n",
      "6 n\n",
      "7 s\n",
      "0 t\n",
      "1 ̪\n",
      "2 h\n",
      "0 p\n",
      "1 h\n",
      "0 x\n",
      "1 b\n",
      "2 r\n",
      "3 w\n",
      "0 k\n",
      "1 e\n",
      "0 k\n",
      "1 w\n",
      "2 l\n",
      "3 e\n"
     ]
    }
   ],
   "source": [
    "with open('UserInput.txt','r') as file:\n",
    "    RawInput = file.read()\n",
    "    \n",
    "print(str(RawInput))\n",
    "Object =  TTS(RawInput)\n",
    "Object.Testing(RawInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74202544-f6cf-4cda-891e-477cff46f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "import boto3\n",
    "\n",
    "\n",
    "import boto3\n",
    "from boto3 import Session\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from contextlib import closing\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from tempfile import gettempdir\n",
    "\n",
    "polly = boto3.client(\"polly\",aws_access_key_id='AKIATTX7A56JGNAY2HE2',\n",
    "                     aws_secret_access_key='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',region_name ='us-east-1')\n",
    "\n",
    "#     # get submitted text string and selected voice\n",
    "#     text = event[\"text\"]\n",
    "#     voice = event.get(\"voice\", \"Salli\")\n",
    "\n",
    "#     # strip out slashes if submitted with text string\n",
    "#     if \"/\" in text:\n",
    "#         text = text.replace(\"/\", \"\")\n",
    "\n",
    "#     # generate phoneme tag for polly to read\n",
    "#     phoneme = f\"<phoneme alphabet='ipa' ph='kʌ.təl.fɪʃ'></phoneme>\"\n",
    "\n",
    "#     # send to polly, requesting mp3 back\n",
    "#     response = polly.synthesize_speech(\n",
    "#         OutputFormat=\"mp3\",\n",
    "#         TextType=\"ssml\",\n",
    "#         Text=phoneme,\n",
    "#         VoiceId=voice\n",
    "#     )\n",
    "\n",
    "try:\n",
    "    response = polly.synthesize_speech(Text=\"Access the audio stream from the response\", OutputFormat=\"mp3\", VoiceId=\"Joanna\")\n",
    "except (BotoCoreError, ClientError) as error:\n",
    "    print(error)\n",
    "    \n",
    "    \n",
    "# # Access the audio stream from the response\n",
    "# if \"AudioStream\" in response:\n",
    "#         with closing(response[\"AudioStream\"]) as stream:\n",
    "#             output = os.path.join(gettempdir(), \"speech.mp3\")\n",
    "#             try:\n",
    "#             # Open a file for writing the output as a binary stream\n",
    "#                 with open(output, \"wb\") as file:\n",
    "#                     file.write(stream.read())\n",
    "#             except IOError as error:\n",
    "#             # Could not write to file, exit gracefully\n",
    "#                 print(error)\n",
    "#                 sys.exit(-1)\n",
    "# else:\n",
    "#     # The response didn't contain audio data, exit gracefully\n",
    "#     print(\"Could not stream audio\")\n",
    "#     sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e7c7a0-f4a0-4ff9-b22e-87d5640351f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'c27797bf-3f57-4b21-9ce3-4744a85e65c4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c27797bf-3f57-4b21-9ce3-4744a85e65c4', 'x-amzn-requestcharacters': '41', 'content-type': 'audio/mpeg', 'transfer-encoding': 'chunked', 'date': 'Fri, 01 Apr 2022 14:23:45 GMT'}, 'RetryAttempts': 0}, 'ContentType': 'audio/mpeg', 'RequestCharacters': 41, 'AudioStream': <botocore.response.StreamingBody object at 0x7fe3934023d0>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d8fa59-9955-4502-baf7-878b21832068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a '.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919e973-15dd-41c5-8577-b708e6c65e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
