{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "326226b7-2e34-4893-acd5-4a8a826f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8fc68d-339b-497a-b490-917cfa7572b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : استاد has IPA : ['ɑ', 's', 't̪', 'ɑ', 'd̪']\n",
      "ر Not present :\n",
      "token : شاګرد has IPA : ['ʃ', 'ɑ', 'ɡ', None, 'd̪']\n",
      "token : ته has IPA : ['t̪', 'h']\n",
      "token : اووې has IPA : ['ɑ', 'w', 'w', 'e']\n",
      "token : چه has IPA : ['t͡ʃ', 'h']\n",
      "token : غوږ has IPA : ['ɣ', 'w', 'ʐ']\n",
      "token : اونيسه has IPA : ['ɑ', 'w', 'n', 'i', 's', 'h']\n",
      "Sentance :استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$%\n",
      "Normalized Sentance :استاد شاګرد ته اووې چه غوږ اونيسه  \n",
      "TokenizedText :['استاد', 'شاګرد', 'ته', 'اووې', 'چه', 'غوږ', 'اونيسه'])\n"
     ]
    }
   ],
   "source": [
    "class TTS:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Sentance = self.UserInput()\n",
    "        \n",
    "        self.Normalized = self.Normalization(self.Sentance)\n",
    "        \n",
    "        self.Tokenized = self.Tokenization(self.Normalized)\n",
    "        \n",
    "        self.SentanceSyllables = self.Syllables(self.Tokenized)\n",
    "        # self.TaggedSentance = self.Tagging(self.Tokenized)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Sentance :{}\\nNormalized Sentance :{}\\nTokenizedText :{})\".format(self.Sentance,self.Normalized,self.Tokenized)\n",
    "        \n",
    "    def UserInput(self):\n",
    "        \n",
    "        # for the time being store the sentance in files and read from that file\n",
    "        try: \n",
    "            try: \n",
    "                \n",
    "                Sent = open(\"UserInput.txt\" ,mode = 'r' , encoding='utf-8').read()\n",
    "                return Sent\n",
    "            except:\n",
    "                print(\"UserInput file is empty \")\n",
    "\n",
    "        except:\n",
    "            \n",
    "            print(\"Inputs are not defined in UserInput().\")\n",
    "            \n",
    "    def Normalization(self,Sentance ,encoding='utf-8'):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # case 1 [Remove English character from Sentance]\n",
    "        DeletedEngChar = re.sub('[a-zA-Z]' ,\"\",Sentance)\n",
    "        \n",
    "        # case 2 [Remove Special character from DeletedEngChar]\n",
    "        \n",
    "        DeletedSpecialChar = re.sub( '[~!@#$%^&*()-_+]' , \"\",DeletedEngChar )\n",
    "        \n",
    "        # Can add more Usecases it here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # case 3 [Replace Counting digit to Pashto couting words]\n",
    "        \n",
    "        MathDigit = ['1','2','3','4','5','6','7','8','9','0']\n",
    "        \n",
    "        # iterate over all Math digit and replace with it\n",
    "        pasto_counting_dict = {\n",
    "           '1': \"يو\"\n",
    "             ,\n",
    "             '2': \"دؤه\",\n",
    "             '3':\"    درے\",\n",
    "             '4':\"څلور\",\n",
    "             '5':\"    پنځه\",\n",
    "             '6':\"شپږ\",\n",
    "             '7':\"أوؤه\",\n",
    "             '8':\"    أته\",\n",
    "             '9':\"نهه\",\n",
    "             '0' : \"صفر\"}\n",
    "        \n",
    "        \n",
    "        for digit in MathDigit:\n",
    "            DeletedSpecialChar = DeletedSpecialChar.replace(digit ,pasto_counting_dict[digit] )\n",
    "         \n",
    "        return DeletedSpecialChar\n",
    "        \n",
    "    def Tokenization(self,Sentance):\n",
    "        \n",
    "        \n",
    "        # case 1 [Removed extra spavce from the Sentance]\n",
    "        \n",
    "        FreeExtraSpaceText= Sentance.strip()\n",
    "        \n",
    "\n",
    "        # case 2 [Tokenization] from Python lib's\n",
    "        \n",
    "        TokenizedText = word_tokenize(FreeExtraSpaceText)\n",
    "        \n",
    "        \n",
    "        return TokenizedText\n",
    "    \n",
    "    \n",
    "    def Tagging(self,TokenizeTextList):\n",
    "        \n",
    "        pass\n",
    "        # for word in TokenizeTextList:\n",
    "            # print(word)\n",
    "            \n",
    "            \n",
    "    def Syllables(self,SentanceTokens):\n",
    "        \n",
    "        for index , token in enumerate(SentanceTokens):\n",
    "            # print(index,token)\n",
    "            print(\"token : {} has IPA : {}\".format(token,self.IpaOfToken(token)))\n",
    "    \n",
    "    def IpaOfToken(self,token):\n",
    "        \n",
    "        tokenIpalist = []\n",
    "        \n",
    "        for char in token:\n",
    "            CharIPA = self.IPA(char)\n",
    "            # print(\"Char : {}\\nIPA : {}\".format(char,CharIPA) )\n",
    "            tokenIpalist.append(CharIPA)\n",
    "            \n",
    "        return tokenIpalist\n",
    "            \n",
    "        \n",
    "    def IPA(self,TokenChar):\n",
    "        df = pd.read_csv(\"Datasets/IPA_pashto.csv\",encoding=\"utf-8\")\n",
    "\n",
    "        Final = list(df[\"Final\"])\n",
    "        Medial = list(df[\"Medial\"])\n",
    "        Initial = list(df[\"Initial\"])\n",
    "        Isolated= list(df[\"Isolated\"])\n",
    "        IPA = list(df[\"IPA\"])\n",
    "        \n",
    "        \n",
    "        IPA_dic = {}\n",
    "    \n",
    "        for index in range(0,len(IPA)):\n",
    "            IPA_dic[Isolated[index]] = IPA[index]\n",
    "            IPA_dic[Initial[index]] = IPA[index]\n",
    "            IPA_dic[Medial[index]] = IPA[index]\n",
    "            IPA_dic[Final[index]] = IPA[index]\n",
    "            \n",
    "            \n",
    "            \n",
    "        if TokenChar in IPA_dic:\n",
    "            return IPA_dic[TokenChar]\n",
    "        else:\n",
    "            f = open(\"Datasets/not_available_ipa.txt\",\"+w\")\n",
    "            f.write(TokenChar)\n",
    "            f.close()\n",
    "            print(\"{} Not present :\".format(TokenChar))\n",
    "        return\n",
    "\n",
    "        \n",
    "\n",
    "tts = TTS()\n",
    "print(tts)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c09ee622-844f-4987-ae1c-558b004de157",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pprint() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157046/2722721963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mobl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinguistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# obl.IPA(['استاد', 'شاګرد', 'ته', 'اووې', 'چه', 'غوږ', 'اونيسه'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mobl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_ipa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ر'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_157046/2722721963.py\u001b[0m in \u001b[0;36mdf_ipa\u001b[0;34m(self, TokenChar)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mfinalDic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIPA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Dic : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinalDic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mIPA_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pprint() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "class linguistic:\n",
    "    \n",
    "    \n",
    "    def IPA(self,token_list):\n",
    "        filledIPA = []\n",
    "        for token in token_list:\n",
    "            IPAString = \"\"\n",
    "            for char in token:\n",
    "                \n",
    "                ret = self.df_ipa(char)\n",
    "                print(\"Token : {} ,char : {} , IPA : {} \".format(token,char,ret))\n",
    "                IPAString = IPAString+str(ret)\n",
    "            filledIPA.append(IPAString)\n",
    "        print('filledIPA : ',filledIPA)\n",
    "            \n",
    "        \n",
    "    def df_ipa(self,TokenChar):\n",
    "        '''\n",
    "            There are three type dictonary Final, medial and initial and \n",
    "        '''\n",
    "        df = pd.read_csv(\"Datasets/IPA_pashto.csv\",encoding=\"utf-8\")\n",
    "\n",
    "        Final = list(df[\"Final\"])\n",
    "        Medial = list(df[\"Medial\"])\n",
    "        Initial = list(df[\"Initial\"])\n",
    "        Isolated= list(df[\"Isolated\"])\n",
    "        IPA = list(df[\"IPA\"])\n",
    "        \n",
    "        \n",
    "        finalDic = {}\n",
    "        for index ,char in enumerate(range(0,len(Final))):\n",
    "            finalDic[Final[char]] = IPA[index]\n",
    "            \n",
    "        pp.pprint(\"Final Dic : \",finalDic)\n",
    "        \n",
    "        IPA_dic = {}\n",
    "    \n",
    "        for index in range(0,len(IPA)):\n",
    "            IPA_dic[Isolated[index]] = IPA[index]\n",
    "            IPA_dic[Initial[index]] = IPA[index]\n",
    "            IPA_dic[Medial[index]] = IPA[index]\n",
    "            IPA_dic[Final[index]] = IPA[index]\n",
    "            \n",
    "            \n",
    "            \n",
    "        if TokenChar in IPA_dic:\n",
    "            return IPA_dic[TokenChar]\n",
    "        else:\n",
    "            f = open(\"Datasets/not_available_ipa.txt\",\"+w\")\n",
    "            f.write(TokenChar)\n",
    "            f.close()\n",
    "            print(\"{} Not present :\".format(TokenChar))\n",
    "        return\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    obl = linguistic()\n",
    "    # obl.IPA(['استاد', 'شاګرد', 'ته', 'اووې', 'چه', 'غوږ', 'اونيسه'])\n",
    "    obl.df_ipa('ر')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c539a23-0537-4ef4-9d8e-1e7bd39c8422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Text :استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$% \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextPreProcessing' object has no attribute 'Testing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157046/433371094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mrandomINput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$%\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mProObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextPreProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomINput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mProObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrginalText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextPreProcessing' object has no attribute 'Testing'"
     ]
    }
   ],
   "source": [
    "class TextPreProcessing:\n",
    "    \n",
    "    def __init__(self,OrginalText):\n",
    "        self.OrginalText = OrginalText\n",
    "        print(\"Orginal Text :{} \".format(self.OrginalText))\n",
    "        \n",
    "        \n",
    "    def Cleaning(self,Sentance):\n",
    "        \n",
    "        # case 0 : remove extra space if exist \n",
    "        # RES = Removed Extra spaceed\n",
    "        RES = Sentance.strip()\n",
    "        \n",
    "        \n",
    "        # case 2 : remove English character if exist :\n",
    "        # REC  = Removed English Character\n",
    "        REC = re.sub('[a-zA-Z]' ,\"\",RES)\n",
    "        \n",
    "        # case 3 :  Remove special character:\n",
    "        # RSC = Removed Special characters\n",
    "        RSC = re.sub( '[~!@#$%^&*()-_+]' , \"\",REC )\n",
    "        \n",
    "        # console print \n",
    "        print(\"Cleaned Text : {} \".format(RSC))\n",
    "        \n",
    "        return RSC\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def Tokenization(self,Sentance):\n",
    "        # case 1 [Tokenization] from Python lib's\n",
    "        \n",
    "        \n",
    "        TokenizedText = word_tokenize(Sentance)\n",
    "        \n",
    "        print(\"Tokenized Text : {}\".format(TokenizedText))\n",
    "        \n",
    "        return TokenizedText\n",
    "    \n",
    "    \n",
    "    def Normalization(sefl,sentance):\n",
    "        \n",
    "        # case 3 [Replace Counting digit to Pashto couting words]\n",
    "        \n",
    "        MathDigit = ['1','2','3','4','5','6','7','8','9','0']\n",
    "        \n",
    "        # iterate over all Math digit and replace with it\n",
    "        pasto_counting_dict = {\n",
    "           '1': \"يو\",\n",
    "             '2': \"دؤه\",\n",
    "             '3':\"درے\",\n",
    "             '4':\"څلور\",\n",
    "             '5':\"پنځه\",\n",
    "             '6':\"شپږ\",\n",
    "             '7':\"أوؤه\",\n",
    "             '8':\"أته\",\n",
    "             '9':\"نهه\",\n",
    "             '0' : \"صفر\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for digit in MathDigit:\n",
    "            sentance = sentance.replace(digit ,pasto_counting_dict[digit] )\n",
    "        print(\"Normalized Text : {}\".format(sentance))\n",
    "        \n",
    "        return sentance\n",
    "    def Testing(self,Text):\n",
    "        \n",
    "        # case 1 clean it \n",
    "        CleanedText = self.Cleaning(Text)\n",
    "        \n",
    "        # case 2 : Normalization\n",
    "        NormalaizedText = self.Normalization(CleanedText)\n",
    "        \n",
    "        # case 3 Tokenization\n",
    "        \n",
    "        TokenizedText = self.Tokenization(NormalaizedText)\n",
    "        \n",
    "        print(\"Final Text : \",TokenizedText)\n",
    "        \n",
    "        self.IPA(TokenizedText)\n",
    "    \n",
    "   \n",
    "        \n",
    "    \n",
    "randomINput = \"استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$%\"\n",
    "ProObject = TextPreProcessing(randomINput)\n",
    "ProObject.Testing(ProObject.OrginalText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aea90f-06e8-4bad-bfc0-a91afeda9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linguistic:\n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ff73618-c736-46e0-98e2-9eaed3e0e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTS(TextPreProcessing ,linguistic):\n",
    "    def __init__(self,text):\n",
    "        TextPreProcessing.__init__(self,text)\n",
    "        \n",
    "    def Testing(self,Text):\n",
    "        \n",
    "        # case 1 clean it \n",
    "        CleanedText = self.Cleaning(Text)\n",
    "        \n",
    "        # case 2 : Normalization\n",
    "        NormalaizedText = self.Normalization(CleanedText)\n",
    "        \n",
    "        # case 3 Tokenization\n",
    "        \n",
    "        TokenizedText = self.Tokenization(NormalaizedText)\n",
    "        \n",
    "        print(\"Final Text : \",TokenizedText)\n",
    "        \n",
    "        self.IPA(TokenizedText)\n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e71fab7-1828-426a-9bca-e98641c04aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Text :استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$% \n",
      "Cleaned Text : استاد شاګرد ته اووې چه غوږ اونيسه   \n",
      "Normalized Text : استاد شاګرد ته اووې چه غوږ اونيسه  \n",
      "Tokenized Text : ['استاد', 'شاګرد', 'ته', 'اووې', 'چه', 'غوږ', 'اونيسه']\n",
      "Final Text :  ['استاد', 'شاګرد', 'ته', 'اووې', 'چه', 'غوږ', 'اونيسه']\n",
      "ر Not present :\n",
      "filledIPA :  ['ɑst̪ɑd̪', 'ʃɑɡNoned̪', 't̪h', 'ɑwwe', 't͡ʃh', 'ɣwʐ', 'ɑwnish']\n"
     ]
    }
   ],
   "source": [
    "RawInput = \"استاد شاګرد ت%^&%3ه اووې چه غوږ اونيسه3324 2 s3$%\"\n",
    "Object =  TTS(RawInput)\n",
    "Object.Testing(RawInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74202544-f6cf-4cda-891e-477cff46f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7c7a0-f4a0-4ff9-b22e-87d5640351f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
