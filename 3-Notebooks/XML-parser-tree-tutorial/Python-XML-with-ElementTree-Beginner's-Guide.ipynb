{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982920d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5fee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xml-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "887496fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the xml data\n",
    "# intro to Element Tree\n",
    "\n",
    "tree = ET.parse('movie.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d46f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x7fa4ea65c520>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718b49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'collection' at 0x7fa4ea5ddc70>\n"
     ]
    }
   ],
   "source": [
    "root = tree.getroot()\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "795b5e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'collection'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ffc88c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d88c8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7017856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre :attrib>  {'category': 'Action'}\n",
      "genre :attrib>  {'category': 'Thriller'}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    print(child.tag ,\":attrib> \",child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82f8270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection\n",
      "genre\n",
      "decade\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "decade\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "genre\n",
      "decade\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "decade\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n",
      "movie\n",
      "format\n",
      "year\n",
      "rating\n",
      "description\n"
     ]
    }
   ],
   "source": [
    "for elem in root.iter():\n",
    "    print(elem.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e43e94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favorite': 'True', 'title': 'Indiana Jones: The raiders of the lost Ark'}\n",
      "{'favorite': 'True', 'title': 'THE KARATE KID'}\n",
      "{'favorite': 'False', 'title': 'Back 2 the Future'}\n",
      "{'favorite': 'False', 'title': 'X-Men'}\n",
      "{'favorite': 'True', 'title': 'Batman Returns'}\n",
      "{'favorite': 'False', 'title': 'Reservoir Dogs'}\n",
      "{'favorite': 'False', 'title': 'ALIEN'}\n",
      "{'favorite': 'True', 'title': \"Ferris Bueller's Day Off\"}\n",
      "{'favorite': 'FALSE', 'title': 'American Psycho'}\n"
     ]
    }
   ],
   "source": [
    "for movie in root.iter('movie'):\n",
    "    print(movie.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dff18444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                'Archaeologist and adventurer Indiana Jones \n",
      "                is hired by the U.S. government to find the Ark of the \n",
      "                Covenant before the Nazis.'\n",
      "                \n",
      "None provided.\n",
      "Marty McFly\n",
      "Two mutants come to a private academy for their kind whose resident superhero team must \n",
      "               oppose a terrorist organization with similar powers.\n",
      "NA.\n",
      "WhAtEvER I Want!!!?!\n",
      "\"\"\"\"\"\"\"\"\"\n",
      "Funny movie about a funny guy\n",
      "psychopathic Bateman\n"
     ]
    }
   ],
   "source": [
    "for description in root.iter('description'):\n",
    "    print(description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dea6c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favorite': 'True', 'title': 'Batman Returns'}\n",
      "{'favorite': 'False', 'title': 'Reservoir Dogs'}\n"
     ]
    }
   ],
   "source": [
    "# X path uses\n",
    "for movie in root.findall(\"./genre/decade/movie/[year='1992']\"):\n",
    "    print(movie.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a61a5965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ba5e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words {}\n"
     ]
    }
   ],
   "source": [
    "root = tree.getroot()\n",
    "print(root.tag , root.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f9c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tree = ET.parse(\"Pashto-context.xml\")\n",
    "\n",
    "\n",
    "sent = open(\"frequent-pasto-datasets.txt\",encoding ='utf8').read()\n",
    "\n",
    "token = word_tokenize(sent)\n",
    "\n",
    "for word in token:\n",
    "    root = tree.getroot()\n",
    "    count = 0 \n",
    "    for child in root:\n",
    "        nodetxt = child.find('word').text\n",
    "        \n",
    "        if nodetxt == word:\n",
    "            print(word)\n",
    "            count+=1\n",
    "            \n",
    "    \n",
    "\n",
    "# print(\"Sentance : \" ,sent)\n",
    "# for child in root:\n",
    "#     print(child.find('word').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30928796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character in file : 309397\n",
      "number of line :  9135\n",
      "Your input file has characters = 309397\n",
      "Your input file has lines = 9135\n",
      "Your input file has the following words = 65626\n",
      "\n",
      " The 30 most frequent words are /n\n",
      " 1. 1375 و\n",
      " 2. 1142 در\n",
      " 3. 1127 د\n",
      " 4. 1016 را\n",
      " 5.  850 که\n",
      " 6.  763 از\n",
      " 7.  738 په\n",
      " 8.  553 به\n",
      " 9.  501 قصه\n",
      "10.  366 شاگردان\n",
      "11.  343 کې\n",
      "12.  318 او\n",
      "13.  317 تا\n",
      "14.  308 له\n",
      "15.  306 ته\n",
      "16.  266 هم\n",
      "17.  263 خود\n",
      "18.  254 با\n",
      "19.  246 چې\n",
      "20.  226 روز\n",
      "21.  225 ها\n",
      "22.  225 ای\n",
      "23.  211 نه\n",
      "24.  209 یک\n",
      "25.  208 کلمات\n",
      "26.  186 آن\n",
      "27.  185 مورد\n",
      "28.  177 هر\n",
      "29.  167 پا\n",
      "30.  166 ید\n",
      "Thank You! Goodbye.\n"
     ]
    }
   ],
   "source": [
    "pashto_file = open(\"pashto-corpus.txt\",encoding='utf8').read()\n",
    "\n",
    "num_char = len(pashto_file)\n",
    "print(\"character in file :\",num_char)\n",
    "\n",
    "# Program will count the lines in the text file\n",
    "num_lines = pashto_file.count('\\n')\n",
    "\n",
    "print(\"number of line : \",num_lines)\n",
    "\n",
    "words =  pashto_file.split()\n",
    "\n",
    "d = {} # occourance of the words\n",
    "\n",
    "for w in words:\n",
    "    if w in d:\n",
    "        d[w] +=1\n",
    "    else:\n",
    "        d[w] =1\n",
    "        \n",
    "num_words = sum(d[w] for w in d)\n",
    "\n",
    "lst = [(d[w],w) for w in d]\n",
    "lst.sort()\n",
    "lst.reverse()\n",
    "# print(lst)\n",
    "\n",
    "\n",
    "# Program will print the results\n",
    "print('Your input file has characters = '+str(num_char))\n",
    "print('Your input file has lines = '+str(num_lines))\n",
    "print('Your input file has the following words = '+str(num_words))\n",
    "\n",
    "print('\\n The 30 most frequent words are /n')\n",
    "\n",
    "\n",
    "i = 1\n",
    "for count, word in lst[:30]:\n",
    "    print('%2s. %4s %s' %(i,count,word))\n",
    "    i+= 1\n",
    "\n",
    "\n",
    "print(\"Thank You! Goodbye.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d56d4cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse line 2: اسس -> اَس  | ح اَس | ح اَس اَس | اَس اَس | اسف\nExpected a nonterminal, found: َس  | ح اَس | ح اَس اَس | اَس اَس | اسف",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1430\u001b[0m                 \u001b[0;31m# expand out the disjunctions on the RHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m                 \u001b[0mproductions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_read_production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36m_read_production\u001b[0;34m(line, nonterm_parser, probabilistic)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0mnonterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             \u001b[0mrhsides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mstandard_nonterm_parser\u001b[0;34m(string, pos)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a nonterminal, found: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a nonterminal, found: َس  | ح اَس | ح اَس اَس | اَس اَس | اسف",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1521380/679761335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtokenize_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m grammar1 = nltk.CFG.fromstring(\"\"\"\n\u001b[0m\u001b[1;32m     28\u001b[0m    \u001b[0mاسس\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m  \u001b[0;34m|\u001b[0m \u001b[0mح\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mح\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m \u001b[0mا\u001b[0m\u001b[0;31mَ\u001b[0m\u001b[0mس\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mاسف\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m  \u001b[0mس\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mفس\u001b[0m \u001b[0mاسس\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mfromstring\u001b[0;34m(cls, input, encoding)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meither\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         start, productions = read_grammar(\n\u001b[0m\u001b[1;32m    547\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_nonterm_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0mproductions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_read_production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to parse line {linenum + 1}: {line}\\n{e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse line 2: اسس -> اَس  | ح اَس | ح اَس اَس | اَس اَس | اسف\nExpected a nonterminal, found: َس  | ح اَس | ح اَس اَس | اَس اَس | اسف"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.generate import generate\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "path = \"/home/iffishells/Pictures/Pushto-TTS-FYP/MS-thesis/dict\"\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "#     print(filename)\n",
    "    tree = etree.parse(path+\"/\"+filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for child in root:\n",
    "        nodetext = child.find('word').text\n",
    "        \n",
    "        if (nodetext ==-1):\n",
    "            test = child.findtext(\"./int/gram\")\n",
    "            test = test.replace(\"[\",\"\")\n",
    "            test = test.replace(\"]\",\"\")\n",
    "            test = test.replace(\"-\",\"\")\n",
    "            test = test.replace(\"=\",\"\")\n",
    "            \n",
    "            tokenize_sent = word_tokenize(test)\n",
    "            \n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "   اسس -> اَس  | ح اَس | ح اَس اَس | اَس اَس | اسف\n",
    " س -> فس اسس\n",
    " اسس -> اَس  | ح اَس | ح اَس اَس | اَس اَس\n",
    " فس -> ف | ح ف | ف ح | ص ف | اَس ح ف | اَس ح ح ف\n",
    " اسف\" -> عډد اَس\"\n",
    " پپ ->  اَس ح | ح اَس | ح اَس ح\n",
    " \n",
    " اسس -> اَس  | ح اَس | ح اَس اَس | اَس اَس\n",
    "\n",
    "AP -> H N | NN N\n",
    "N -> path\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "sen = \"\"\"ګودرغاړه\n",
    "عقل\n",
    "اولاړشو\n",
    "غاړه\n",
    "د افلاكو علم\n",
    "د انشاء علم\"\"\"\n",
    "sent = sen.split()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29a01691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.parse.shiftreduce.ShiftReduceParser at 0x7fa478e635b0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " VP -> V NP | V NP PP | ADJ V | ADJ NP V \n",
    " PP -> P NP\n",
    " V -> \"saw\" | \"ate\" | \"walked\" | \"شو\"\n",
    " NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP | N\n",
    " Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    " N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" | \"apple\" | \"يوسف\"\n",
    " P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    " ADJ -> \"غلے\" | \"is\"\n",
    " \"\"\")\n",
    "\n",
    "#sr_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "sen = 'the dog saw a man on the park'\n",
    "sent = sen.split()\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)\n",
    "\n",
    "\n",
    "\n",
    "sr_parse = nltk.ShiftReduceParser(grammar1, trace=2)\n",
    "\n",
    "sr_parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3957d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance  نه زه به سبا ته وخته پا نه څېږممذ \n",
      "Tokensize sentenace  ['نه', 'زه', 'به', 'سبا', 'ته', 'وخته', 'پا', 'نه', 'څېږممذ']\n",
      "0 Token :  نه\n",
      "1 Token :  زه\n",
      "2 Token :  به\n",
      "3 Token :  سبا\n",
      "4 Token :  ته\n",
      "5 Token :  وخته\n",
      "6 Token :  پا\n",
      "7 Token :  نه\n",
      "8 Token :  څېږممذ\n"
     ]
    }
   ],
   "source": [
    "def tagging():\n",
    "    sen = 'نه زه به سبا ته وخته پا نه څېږممذ '\n",
    "    print(\"Sentance \",sen)\n",
    "    print(\"Tokensize sentenace \" ,word_tokenize(sen))\n",
    "    \n",
    "    path = \"/home/iffishells/Pictures/Pushto-TTS-FYP/MS-thesis/dict/\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index ,token in enumerate(word_tokenize(sen)):\n",
    "        \n",
    "        print(index ,\"Token : \",token)\n",
    "        \n",
    "        for filename in os.listdir(path):\n",
    "            tree = etree.parse(path+filename)\n",
    "            root = tree.getroot()\n",
    "        \n",
    "            for child in root:\n",
    "                nodetext = child.find('word').text\n",
    "\n",
    "                if nodetext == token:\n",
    "                    test=child.findtext(\"./int/gram\")\n",
    "                    test=test.replace(\"[\", \"\")\n",
    "                    test=test.replace(\"]\", \"\")\n",
    "                    test=test.replace(\"-\", \" \")\n",
    "                    test=test.replace(\"=\", \" \")\n",
    "                    test=test.replace(\"(\", \" \")\n",
    "                    test=test.replace(\")\", \" \")\n",
    "                    token_ = word_tokenize(test)\n",
    "\n",
    "                    print(index ,\" inside token __ : \",token)\n",
    "                    \n",
    "                    \n",
    "                    for t in token_:\n",
    "                        print(token , \"\\t\",\"\\t\" , t)\n",
    "                        \n",
    "                        if(t == \"اَس\"):\n",
    "                            print(\"Noun\")\n",
    "                        elif(t == \"مذ\"):\n",
    "                            print(\"Masculine\")\n",
    "                        elif(t == \"ج\"):\n",
    "                            print(\"Plural\")\n",
    "                        elif(t == \"مؤ\"):\n",
    "                            print(\"Femenine\")\n",
    "                        elif(t == \"ص\"):\n",
    "                            print(\"Adjective\")\n",
    "                        elif(t == \"فاعل\"):\n",
    "                            print(\"Adverb\")\n",
    "                        elif(t == \"فعل\"):\n",
    "                            print(\"Verb\")\n",
    "                        elif(t == \"حا\"):\n",
    "                            print(\"Present\")\n",
    "                        elif(t == \"ما\"): \n",
    "                            print(\"Past\")\n",
    "                        elif(t == \"وا\"):\n",
    "                            print(\"Singular\")\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "tagging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "49e3067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from xlrd import open_workbook\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from xlrd import open_workbook\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.tree import *\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import xml.etree.ElementTree as etree\n",
    "import xlrd\n",
    "import time\n",
    "import sys\n",
    "from nltk import induce_pcfg\n",
    "from nltk.parse import pchart\n",
    "from nltk import PCFG\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "442f9f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "XLRDError",
     "evalue": "Excel xlsx file; not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXLRDError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1521380/3458445465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pastho dictionary2.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows, ignore_workbook_corruption)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0munnumbered\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momit\u001b[0m \u001b[0moffsets\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mmeaningful\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbiffh\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbiff_dump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mbk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mbk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiff2_8_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXLRDError\u001b[0m: Excel xlsx file; not supported"
     ]
    }
   ],
   "source": [
    "book = open_workbook('Pastho dictionary2.xlsx')\n",
    "\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77511c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d082ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
